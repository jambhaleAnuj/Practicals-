{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT5eW70ezolR",
        "outputId": "1d4e744f-4296-41fd-e021-9e98a8d1207f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.9\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install -U dill"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence, bigrams, ngrams, everygrams #imports\n",
        "import collections"
      ],
      "metadata": {
        "id": "pKRFFfW7zrx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=[['Jaanu','Meri' ,'Jaan'], ['Mai' ,'Tera' ,'Tu' ,'Meri' ,'Jane' , 'Hindustan' ]] #sample text"
      ],
      "metadata": {
        "id": "MRl0VCRGzx-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(text[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnb3cwGzz5Go",
        "outputId": "0930f1c6-00a4-4657-acd2-7971f085013e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jaanu', 'Meri'), ('Meri', 'Jaan')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(text[1],3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxxKfCXe0F_P",
        "outputId": "85aa402c-1847-43a0-d503-fb09cf7942bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mai', 'Tera', 'Tu'),\n",
              " ('Tera', 'Tu', 'Meri'),\n",
              " ('Tu', 'Meri', 'Jane'),\n",
              " ('Meri', 'Jane', 'Hindustan')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.util import pad_sequence\n",
        "padded_bigrams=list(pad_sequence(text[0],2,pad_left=True, pad_right=True, left_pad_symbol='<s>' , right_pad_symbol='</s>'))"
      ],
      "metadata": {
        "id": "EG7V3YMd0SW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(bigrams(padded_bigrams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjXnU4-B0Y6w",
        "outputId": "5113ed86-9424-4b6b-8c03-fc3a5fc25794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'Jaanu'), ('Jaanu', 'Meri'), ('Meri', 'Jaan'), ('Jaan', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_trigrams=list(pad_sequence(text[1],3, pad_left=True, pad_right=True, left_pad_symbol='<s>' , right_pad_symbol='</s>' ) )"
      ],
      "metadata": {
        "id": "sayLNdfR0jrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(padded_trigrams, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33_4m-t0k-b",
        "outputId": "e541d126-14f3-4dd3-90c0-08cc9218d724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', '<s>', 'Mai'),\n",
              " ('<s>', 'Mai', 'Tera'),\n",
              " ('Mai', 'Tera', 'Tu'),\n",
              " ('Tera', 'Tu', 'Meri'),\n",
              " ('Tu', 'Meri', 'Jane'),\n",
              " ('Meri', 'Jane', 'Hindustan'),\n",
              " ('Jane', 'Hindustan', '</s>'),\n",
              " ('Hindustan', '</s>', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
        "ngrams = list(everygrams(padded_bigrams, max_len=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92P-od4Z0rBq",
        "outputId": "42a10728-db54-4cba-833d-b039d61af47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "#USE ONLY IF THE BELOW COMMENTED CODE DOESNT WORK\n",
        "def custom_padded_everygram_pipeline(order, text_list):\n",
        "# Pad the text and generate n-grams for each text in the text_list\n",
        "  padded_text_list = [pad_both_ends(text, n=order) for text in text_list]\n",
        "  ngrams_list = [list(everygrams(padded_text, max_len=order)) for padded_text in padded_text_list]\n",
        "\n",
        "# Flatten the list of n-grams and return\n",
        "  flattened_ngrams = list(chain.from_iterable(ngrams_list))\n",
        "  return flattened_ngrams"
      ],
      "metadata": {
        "id": "8_u2ToLq0vZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "train_ngrams, vocab = padded_everygram_pipeline(2, text)\n",
        "\n",
        "for i in train_ngrams:\n",
        "  print(list(i))\n",
        "  print()\n",
        "\n",
        "print(list(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPic3NhH03F5",
        "outputId": "669da1ec-9667-436f-fd45-5756eff3ed7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<s>',), ('<s>', 'Jaanu'), ('Jaanu',), ('Jaanu', 'Meri'), ('Meri',), ('Meri', 'Jaan'), ('Jaan',), ('Jaan', '</s>'), ('</s>',)]\n",
            "\n",
            "[('<s>',), ('<s>', 'Mai'), ('Mai',), ('Mai', 'Tera'), ('Tera',), ('Tera', 'Tu'), ('Tu',), ('Tu', 'Meri'), ('Meri',), ('Meri', 'Jane'), ('Jane',), ('Jane', 'Hindustan'), ('Hindustan',), ('Hindustan', '</s>'), ('</s>',)]\n",
            "\n",
            "['<s>', 'Jaanu', 'Meri', 'Jaan', '</s>', '<s>', 'Mai', 'Tera', 'Tu', 'Meri', 'Jane', 'Hindustan', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests #importing corpus\n",
        "import io\n",
        "\n",
        "url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "text=requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt','w', encoding='utf-8') as fout:\n",
        "  fout.write(text)\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokenized_text=[list(map(str.lower,word_tokenize(sent)))\n",
        "for sent in sent_tokenize(text)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AINoXeck09KP",
        "outputId": "2584b56d-6564-4276-d0d7-3efea487a3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8jkStc01GXA",
        "outputId": "722eb3d1-f9fe-4824-d3eb-5ad63b6c6423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language',\n",
              " 'is',\n",
              " 'never',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'ever',\n",
              " ',',\n",
              " 'random',\n",
              " 'adam',\n",
              " 'kilgarriff',\n",
              " 'abstract',\n",
              " 'language',\n",
              " 'users',\n",
              " 'never',\n",
              " 'choose',\n",
              " 'words',\n",
              " 'randomly',\n",
              " ',',\n",
              " 'and',\n",
              " 'language',\n",
              " 'is',\n",
              " 'essentially',\n",
              " 'non-random',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXt4GQXQ1QOZ",
        "outputId": "80595bc2-c8ff-4f0a-e662-c2c95b6dbe1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.corpus import reuters\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Function to create a trigram model\n",
        "def generate_trigrams(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
        "\n",
        "    # Create trigrams\n",
        "    trigram_model = list(trigrams(words))\n",
        "\n",
        "    return trigram_model\n",
        "\n",
        "# Function to generate text based on the trigram model\n",
        "def generate_sentence(trigram_model, seed, length=10):\n",
        "    generated_text = list(seed)  # Start with the seed\n",
        "\n",
        "    for _ in range(length - 2):  # We already have two words\n",
        "        # Look for the trigram starting with the last two words in generated_text\n",
        "        last_two_words = tuple(generated_text[-2:])\n",
        "\n",
        "        # Find the possible next words\n",
        "        next_words = [trigram[2] for trigram in trigram_model if trigram[0] == last_two_words[0] and trigram[1] == last_two_words[1]]\n",
        "\n",
        "        if not next_words:\n",
        "            break\n",
        "\n",
        "        # Randomly pick the next word\n",
        "        next_word = random.choice(next_words)\n",
        "        generated_text.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Example text (you can replace this with any text corpus)\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) concerned with\n",
        "the interactions between computers and human language, and, in particular, concerned with\n",
        "programming computers to fruitfully process large natural language data.\n",
        "\"\"\"\n",
        "\n",
        "# Generate trigrams from the example text\n",
        "trigram_model = generate_trigrams(text)\n",
        "\n",
        "# Seed to start the sentence (two words to start)\n",
        "seed = (\"human\", \"language\")\n",
        "\n",
        "# Generate a sentence\n",
        "generated_sentence = generate_sentence(trigram_model, seed)\n",
        "print(generated_sentence)\n"
      ],
      "metadata": {
        "id": "iUuivTfp1WFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff1fa1f-d951-460d-d09a-d9f1bdf2c5d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human language , and , in particular , concerned with\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yAIecm4Ujld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}